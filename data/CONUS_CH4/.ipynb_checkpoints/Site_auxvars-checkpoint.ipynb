{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap, cm\n",
    "from netCDF4 import Dataset\n",
    "from scipy.interpolate import interp1d\n",
    "import D14Cpreprocess as prep\n",
    "import auxiliary_lib as au\n",
    "import isamcalc_lib as isam\n",
    "import subprocess\n",
    "import socplot_lib as socplt\n",
    "import Fluxtools as flux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#======================================================\n",
    "## Obtain the water table depth data\n",
    "#======================================================\n",
    "totlen = 403248     # From 1995 to 2017\n",
    "totyrrange = 1995 + np.arange(23)\n",
    "day_of_year = [365, 366, 365, 365, 365, 366, 365, 365, 365, 366, 365, \\\n",
    "               365, 365, 366, 365, 365, 365, 366, 365, 365, 365, 366, 365]   # DOY from 1995 to 2017\n",
    "days_of_m = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "slices_of_year = np.dot(48,day_of_year)\n",
    "for i in np.arange(1,len(slices_of_year)):\n",
    "    slices_of_year[i] = slices_of_year[i] + slices_of_year[i-1]\n",
    "\n",
    "# Determine the data length\n",
    "pid = np.ones(totlen)\n",
    "idx = 0\n",
    "for i in np.arange(0,totlen):\n",
    "    idx = idx + 1\n",
    "    pid[i] = idx\n",
    "\n",
    "yr = np.ones(totlen)\n",
    "ptyear = 0\n",
    "for i in np.arange(0,totlen):\n",
    "    yr[i] = totyrrange[ptyear]\n",
    "    if (i >= slices_of_year[ptyear] - 1):\n",
    "        ptyear = ptyear + 1\n",
    "        \n",
    "doy = np.ones(totlen)\n",
    "hr = np.ones(totlen)\n",
    "accuday = 0\n",
    "for i in np.arange(0,len(day_of_year)):\n",
    "    ptday = 0\n",
    "    for j in np.arange(0,day_of_year[i]):\n",
    "        ptday = ptday + 1\n",
    "        for k in np.arange(0,48):\n",
    "            doy[k+accuday*48] = ptday\n",
    "            hr[k+accuday*48] = k/2. + 0.25\n",
    "        accuday = accuday + 1\n",
    "    \n",
    "#======================================================\n",
    "## Read the observation of FCH4\n",
    "#======================================================\n",
    "varname1 = 'WTD'\n",
    "varname2 = 'WATER_TABLE_DEPTH'\n",
    "\n",
    "# US-WPT site data (OLD AMF data accessed from ORNL FTP site)\n",
    "# 30m dataset\n",
    "# Unit = m\n",
    "sitename = ['USWPT']\n",
    "wtd_uswpt = np.ones(totlen)*(-9999.)\n",
    "for i in sitename:\n",
    "    cmd = \"ls -1 AMF_\"+i+\"_*.csv\"\n",
    "    flist = subprocess.check_output(cmd, shell=True).splitlines()\n",
    "    yrrange_uswpt=np.zeros(len(flist))\n",
    "    for j in np.arange(0,len(flist)):\n",
    "        yrrange_uswpt[j] = int(flist[j][10:14])\n",
    "        data = pd.read_csv(flist[j], encoding='iso-8859-1')\n",
    "        WTD = data.WATER_TABLE_DEPTH.as_matrix()   # convert to nmol\n",
    "        obs_date = data.TIMESTAMP.as_matrix() + 1500      \n",
    "        # Coordinate the time period of FCH4 timeseries for US-WPT\n",
    "        yr_site = np.floor(obs_date/1e10)\n",
    "        dd = np.floor((obs_date/1e10-np.floor(obs_date/1e10))*1e4)\n",
    "        hh = np.round((obs_date/1e6 -np.floor(obs_date/1e6))*1e2)\n",
    "        mm = np.round((obs_date/1e4 -np.floor(obs_date/1e4))*1e2)/60.\n",
    "        mo = np.floor(dd/1e2).astype(\"int\")\n",
    "        hr_site=hh+mm\n",
    "        # Get the first time step of the timeseries from file\n",
    "        sitelen = len(WTD)\n",
    "        pt = 0\n",
    "        yr_site_pt = yr_site[pt].astype(\"int\")\n",
    "        doy_site_pt = int((mo[pt]-1)*days_of_m[mo[pt]] + ((dd[pt]/1e2-np.floor(dd[pt]/1e2))*1e2))\n",
    "        hr_site_pt = hr_site[pt]\n",
    "        # Place into the array\n",
    "        for k in np.arange(0,totlen):\n",
    "            if(yr_site_pt == yr[k] and doy_site_pt == doy[k] and (hr_site_pt-hr[k])<1e-6):\n",
    "                idx = k\n",
    "                wtd_uswpt[idx:idx+sitelen] = WTD\n",
    "\n",
    "            \n",
    "# EFDC EC data\n",
    "# 30m dataset\n",
    "# Totally 4 sites are available\n",
    "# Unit = nmol m-2 s-1\n",
    "sitename = ['DESfN']\n",
    "wtd_desfn = np.ones(totlen)*(-9999.)\n",
    "cmd = \"ls -1 EFDC_L2_Flx_DESfN_2012_v03_30m.txt\"\n",
    "flist = subprocess.check_output(cmd, shell=True).splitlines()\n",
    "yrrange_desfn=np.zeros(len(flist))\n",
    "for j in np.arange(0,len(flist)):\n",
    "    yrrange_desfn[j] = int(flist[j][18:22])\n",
    "    data = pd.read_csv(flist[j], encoding='iso-8859-1')\n",
    "    WTD = -100.*data.WTD.as_matrix()\n",
    "    WTD[WTD<-300] = -9999.\n",
    "    WTD[WTD>300] = -9999.\n",
    "    timest = data.TIMESTAMP_START.as_matrix()\n",
    "    timeend = data.TIMESTAMP_END.as_matrix()\n",
    "    sel = ((timeend/1e2-np.floor(timeend/1e2))*1e2 == 0)\n",
    "    timeend[sel] = timeend[sel] - 40\n",
    "    sel = (np.round((timeend/1e4-np.floor(timeend/1e4))*1e4) == 9960)\n",
    "    timeend[sel] = timeend[sel] - 7600\n",
    "    obs_date = np.floor(timest/1e2)*1e2 + (timest/1e2-np.floor(timest/1e2) + \\\n",
    "                                           timeend/1e2-np.floor(timeend/1e2))*1e2/2.\n",
    "    # Coordinate the time period of FCH4 timeseries for US-WPT\n",
    "    yr_site = np.floor(obs_date/1e8)\n",
    "    dd = np.floor((obs_date/1e8 -np.floor(obs_date/1e8))*1e4)\n",
    "    hh = np.round((obs_date/1e4 -np.floor(obs_date/1e4))*1e2)\n",
    "    mm = np.round((obs_date/1e2 -np.floor(obs_date/1e2))*1e2)/60.\n",
    "    mo = np.floor(dd/1e2).astype(\"int\")\n",
    "    hr_site=hh+mm\n",
    "    # Get the first time step of the timeseries from file\n",
    "    sitelen = len(WTD)\n",
    "    pt = 0\n",
    "    yr_site_pt = yr_site[pt].astype(\"int\")\n",
    "    doy_site_pt = int((mo[pt]-1)*days_of_m[mo[pt]] + ((dd[pt]/1e2-np.floor(dd[pt]/1e2))*1e2))\n",
    "    hr_site_pt = hr_site[pt]\n",
    "    # Place into the array\n",
    "    for k in np.arange(0,totlen):\n",
    "        if(yr_site_pt == yr[k] and doy_site_pt == doy[k] and (hr_site_pt-hr[k])<1e-6):\n",
    "            idx = k\n",
    "            wtd_desfn[idx:idx+sitelen] = WTD\n",
    "        \n",
    "# EFDC EC data\n",
    "# 1hr dataset\n",
    "# Unit = nmol m-2 s-1                    \n",
    "sitename = ['NLHor']\n",
    "wtd_nlhor = np.ones(totlen)*(-9999.)\n",
    "cmd = \"ls -1 EFDC_*\"+sitename[0]+\"_*1hr.txt\"\n",
    "flist = subprocess.check_output(cmd, shell=True).splitlines()\n",
    "yrrange_nlhor=np.zeros(len(flist))\n",
    "for j in np.arange(0,len(flist)):\n",
    "    yrrange_nlhor[j] = int(flist[j][18:22])\n",
    "    data = pd.read_csv(flist[j], encoding='iso-8859-1')\n",
    "    WTD = -100. * data.WTD.as_matrix()\n",
    "    WTD[WTD<-300] = -9999.\n",
    "    WTD[WTD>300] = -9999.\n",
    "    time = data.DTime.as_matrix()\n",
    "    # Coordinate the time period of FCH4 timeseries for US-WPT\n",
    "    yr_site = yrrange_nlhor[j]\n",
    "    dd = np.floor(time)\n",
    "    hh = np.round((time - np.floor(time))*24.).astype('int')\n",
    "    mm = np.round((time - np.floor(time))*24.) - 0.5\n",
    "    hr_site=hh+mm\n",
    "    # Get the first time step of the timeseries from file\n",
    "    pt = 0\n",
    "    yr_site_pt = yr_site\n",
    "    doy_site_pt = int(dd[pt])\n",
    "    hr_site_pt = hr_site[pt]\n",
    "    # Need linear interpolation before using the data\n",
    "    x = np.arange(0,len(time))\n",
    "    x_o = np.arange(0,len(time), 0.5)\n",
    "    f_i = interp1d(x, WTD)\n",
    "    f_x = prep.extrap1d(f_i)\n",
    "    WTD_interped = f_x(x_o)\n",
    "    WTD_interped[WTD_interped<-100] = -9999.0\n",
    "    sitelen = len(WTD_interped)\n",
    "    # Place into the array\n",
    "    for k in np.arange(0,totlen):\n",
    "        if(yr_site_pt == yr[k] and doy_site_pt == doy[k] and (hr_site_pt-hr[k])<1e-6):\n",
    "            idx = k\n",
    "            wtd_nlhor[idx:idx+sitelen] = WTD_interped\n",
    "\n",
    "# Create dataframe to combine all observations\n",
    "d = {'ID': pid, 'YEAR': yr, 'DOY': doy, 'TIME': hr, 'USWPT': wtd_uswpt, \\\n",
    "     'DESfN': wtd_desfn, 'NLHor': wtd_nlhor}\n",
    "methane = pd.DataFrame(data=d)\n",
    "# Write the data as CSV file\n",
    "# methane.to_csv('site_wtd.csv')\n",
    "\n",
    "# I think we shall interpolate the data before using it...\n",
    "# First use MDV interpolation through applying a 5-days window, then replace \n",
    "# all missing values by the mean water table depth\n",
    "x = wtd_uswpt[slices_of_year[yrrange_uswpt[0].astype(\"int\")-1995-1]:slices_of_year[yrrange_uswpt[2].astype(\"int\")-1995]]\n",
    "window = 5  # size of the window\n",
    "wtd_uswpt_nc = flux.mdv(x, 5, 48)\n",
    "wtd_uswpt_nc[wtd_uswpt_nc<-5000.] = np.float(\"nan\")\n",
    "mwtd = np.nanmean(wtd_uswpt_nc)\n",
    "wtd_uswpt_nc[np.isnan(wtd_uswpt_nc)] = mwtd\n",
    "wtd_uswpt_nc[wtd_uswpt_nc < 0] = 0. \n",
    "wtd_uswpt_nc = wtd_uswpt_nc/100.\n",
    "\n",
    "t = 17568\n",
    "x = wtd_desfn[slices_of_year[yrrange_desfn[0].astype(\"int\")-1995-1]:slices_of_year[yrrange_desfn[0].astype(\"int\")-1995-1]+t]\n",
    "window = 5  # size of the window\n",
    "wtd_desfn_nc = flux.mdv(x, 5, 48)\n",
    "wtd_desfn_nc[wtd_desfn_nc<-5000.] = np.float(\"nan\")\n",
    "mwtd = np.nanmean(wtd_desfn_nc)\n",
    "wtd_desfn_nc[np.isnan(wtd_desfn_nc)] = mwtd\n",
    "wtd_desfn_nc[wtd_desfn_nc < 0] = 0.\n",
    "wtd_desfn_nc = wtd_desfn_nc/100.\n",
    "\n",
    "x = wtd_nlhor[slices_of_year[yrrange_nlhor[0].astype(\"int\")-1995-1]:slices_of_year[yrrange_nlhor[2].astype(\"int\")-1995]]\n",
    "window = 5  # size of the window\n",
    "wtd_nlhor_nc = flux.mdv(x, 5, 48)\n",
    "wtd_nlhor_nc[wtd_nlhor_nc<-5000.] = np.float(\"nan\")\n",
    "mwtd = np.nanmean(wtd_nlhor_nc)\n",
    "wtd_nlhor_nc[np.isnan(wtd_nlhor_nc)] = mwtd\n",
    "wtd_nlhor_nc[wtd_nlhor_nc < 0] = 0.\n",
    "wtd_nlhor_nc = wtd_nlhor_nc/100.\n",
    "\n",
    "# Save data into netcdf\n",
    "# ==========================================\n",
    "# US-WPT site water table depth\n",
    "# ==========================================\n",
    "fncname = 'US-WPT_WT.nc'\n",
    "# Or store BD into a new NC file\n",
    "t = len(wtd_uswpt_nc)\n",
    "nc = Dataset(fncname, 'w', format ='NETCDF4_CLASSIC')\n",
    "time = nc.createDimension('time', t) \n",
    "lat = nc.createDimension('lat', 1)\n",
    "lon = nc.createDimension('lon', 1)\n",
    "# Coordinate variables\n",
    "time = nc.createVariable('time', np.float64, ('time',)) \n",
    "# Actual variable\n",
    "table = nc.createVariable('WTD', np.float64, ('time', 'lat','lon'))  \n",
    "# Global Attributes \n",
    "nc.description = 'Water table depth for the site WPT'\n",
    "#nc.history = 'Created ' + time.ctime(time.time())  \n",
    "nc.source = 'Downloaded from Ameriflux site by Shijie'\n",
    "# Variable Attributes\n",
    "time.units = \"seconds since 2011-01-01 00:00:00\"\n",
    "table.units = 'cm'\n",
    "\n",
    "# Assign values\n",
    "time[:] = 1800. * np.arange(0,t)\n",
    "table[:,0,0] = wtd_uswpt_nc\n",
    "\n",
    "# Finilize\n",
    "nc.close()\n",
    "\n",
    "# Duplicate 3 times\n",
    "#temp = np.concatenate((wtd_desfn_nc, wtd_desfn_nc))\n",
    "#temp = np.concatenate((temp, wtd_desfn_nc))\n",
    "# Duplicate \n",
    "temp = np.concatenate((wtd_desfn_nc, wtd_desfn_nc))\n",
    "for k in np.arange(0,33):\n",
    "    temp = np.concatenate((temp, wtd_desfn_nc))\n",
    "\n",
    "# ==========================================\n",
    "# DE-SfN site water table depth\n",
    "# ==========================================\n",
    "fncname = 'DE-SfN_WT.nc'\n",
    "# Or store BD into a new NC file\n",
    "t = len(temp)\n",
    "nc = Dataset(fncname, 'w', format ='NETCDF4_CLASSIC')\n",
    "time = nc.createDimension('time', t) \n",
    "lat = nc.createDimension('lat', 1)\n",
    "lon = nc.createDimension('lon', 1)\n",
    "# Coordinate variables\n",
    "time = nc.createVariable('time', np.float64, ('time',)) \n",
    "# Actual variable\n",
    "table = nc.createVariable('WTD', np.float64, ('time', 'lat','lon'))  \n",
    "# Global Attributes \n",
    "nc.description = 'Water table depth for the site WPT'\n",
    "#nc.history = 'Created ' + time.ctime(time.time())  \n",
    "nc.source = 'Downloaded from Ameriflux site by Shijie'\n",
    "# Variable Attributes\n",
    "time.units = \"seconds since 1981-01-01 00:00:00\"\n",
    "table.units = 'cm'\n",
    "\n",
    "# Assign values\n",
    "time[:] = 1800. * np.arange(0,t)\n",
    "#table[:,0,0] = wtd_desfn_nc\n",
    "table[:,0,0] = temp\n",
    "\n",
    "# Finilize\n",
    "nc.close()\n",
    "\n",
    "# Duplicate 3 times\n",
    "#temp = np.concatenate((wtd_desfn_nc, wtd_desfn_nc))\n",
    "#temp = np.concatenate((temp, wtd_desfn_nc))\n",
    "# Duplicate to fill year 1981 - 2011\n",
    "# Make sure that 2004 - 2006 has the observed WTD.\n",
    "#8182 838485 868788 899091 929394 959697 989900 010203 040506 070809 1011\n",
    "temp = np.concatenate((wtd_nlhor_nc[17568:len(wtd_nlhor_nc)], wtd_nlhor_nc))\n",
    "for k in np.arange(0,8):\n",
    "    temp = np.concatenate((temp, wtd_nlhor_nc))\n",
    "# Final 2 years\n",
    "temp = np.concatenate((temp, wtd_nlhor_nc[0:35088]))\n",
    "\n",
    "# ==========================================\n",
    "# NL-Hor site water table depth\n",
    "# ==========================================\n",
    "fncname = 'NL-Hor_WT.nc'\n",
    "# Or store BD into a new NC file\n",
    "t = len(temp)\n",
    "nc = Dataset(fncname, 'w', format ='NETCDF4_CLASSIC')\n",
    "time = nc.createDimension('time', t) \n",
    "lat = nc.createDimension('lat', 1)\n",
    "lon = nc.createDimension('lon', 1)\n",
    "# Coordinate variables\n",
    "time = nc.createVariable('time', np.float64, ('time',)) \n",
    "# Actual variable\n",
    "table = nc.createVariable('WTD', np.float64, ('time', 'lat','lon'))  \n",
    "# Global Attributes \n",
    "nc.description = 'Water table depth for the site WPT'\n",
    "#nc.history = 'Created ' + time.ctime(time.time())  \n",
    "nc.source = 'Downloaded from Ameriflux site by Shijie'\n",
    "# Variable Attributes\n",
    "time.units = \"seconds since 1981-01-01 00:00:00\"\n",
    "table.units = 'cm'\n",
    "\n",
    "# Assign values\n",
    "time[:] = 1800. * np.arange(0,t)\n",
    "table[:,0,0] = temp\n",
    "\n",
    "# Finilize\n",
    "nc.close()\n",
    "\n",
    "#nc = Dataset(fncname, 'r', format ='NETCDF4_CLASSIC')\n",
    "#lats = nc.variables['lat'][:]  # extract/copy the data\n",
    "#lons = nc.variables['lon'][:]\n",
    "#ch4_isam = nc.variables['ch4_flux'][:] # shape is lat, lon as shown above\n",
    "#nc.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52608"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wtd_nlhor_nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Site info: lat, lon and data availability\n",
    "uspfa_loc = [45.9459, -90.2723]\n",
    "uspfa_yr  = [2010, 2014]\n",
    "uswpt_loc = [41.464639, -82.996157]\n",
    "uswpt_yr  = [2011, 2013]\n",
    "usorv_loc = [40.0201, -83.0183]\n",
    "usorv_yr  = [2011, 2012]\n",
    "usbes_loc = [71.2809, -156.5965]\n",
    "usbes_yr  = [2013, 2014]\n",
    "usivo_loc = [68.4865, -155.7503]\n",
    "usivo_yr  = [2013, 2014]\n",
    "desfn_loc = [47.8064, 11.3275]\n",
    "desfn_yr  = [2012, 2015]\n",
    "fihyy_loc = [61.8474, 24.2948]\n",
    "fihyy_yr  = [2012, 2013]\n",
    "nlhor_loc = [52.24035, 5.071301]\n",
    "nlhor_yr  = [2006, 2009]\n",
    "ruche_loc = [68.61304, 161.34143]\n",
    "ruche_yr  = [2014, 2015]\n",
    "aurfi_loc = [-32.5061, 116.9668]\n",
    "\n",
    "methane = pd.read_csv('site_methane.csv')\n",
    "\n",
    "# Read in the model output and extract the corresponding grid point to compare with the observation.\n",
    "#fncname = 'Global_1DSBGC.bgc-yearly-2d_1920.nc'\n",
    "#nc = Dataset(fncname, 'r', format ='NETCDF4_CLASSIC')\n",
    "#lats = nc.variables['lat'][:]  # extract/copy the data\n",
    "#lons = nc.variables['lon'][:]\n",
    "#ch4_isam = nc.variables['ch4_flux'][:] # shape is lat, lon as shown above\n",
    "#nc.close()\n",
    "\n",
    "# Read site level simulation resylts\n",
    "reader = pd.read_csv('uspfa_ch4.txt')\n",
    "\n",
    "uspfa_isam_yr = [uspfa_yr[0] - 1990, uspfa_yr[1] - 1990]\n",
    "uspfa_isam = reader.FCH4.as_matrix()\n",
    "uspfa_isam = uspfa_isam[uspfa_isam_yr[0]*365:(uspfa_isam_yr[1])*365]\n",
    "uspfa_isam = uspfa_isam*1e9/(12*24*3600)\n",
    "# Calculate daily mean\n",
    "uspfa_obs_yr = [uspfa_yr[0] - 1995, uspfa_yr[1] - 1995]\n",
    "uspfa_obs = methane.USPFa[slices_of_year[uspfa_obs_yr[0]]:slices_of_year[uspfa_obs_yr[1]]]\n",
    "uspfa_obs[uspfa_obs<-100] = np.float(\"nan\")\n",
    "uspfa_obs_daily = np.ones(len(uspfa_isam))*float(\"nan\")\n",
    "for j in np.arange(0,(uspfa_obs_yr[1]-uspfa_obs_yr[0])):\n",
    "    for i in np.arange(0,365):\n",
    "        uspfa_obs_daily[365*j+i] = np.nanmean(uspfa_obs[j*17520+i*48:j*17520+(i+1)*48])\n",
    "\n",
    "# ========================================================\n",
    "# Plot the modeled CH4 fluxes against EC observation\n",
    "# ========================================================\n",
    "Xobs = np.arange(0,len(uspfa_obs_daily))\n",
    "Yobs = uspfa_obs_daily\n",
    "Xmod = Xobs\n",
    "Ymod = uspfa_isam\n",
    "tit = \"CH4 flux\"\n",
    "path = \"./uspfa.jpg\"\n",
    "\n",
    "status = socplt.plot_obsvsmod(Xobs, Yobs, Xmod, Ymod, tit, path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
