{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap, cm\n",
    "from netCDF4 import Dataset\n",
    "from scipy.interpolate import interp1d\n",
    "import auxiliary_lib as au\n",
    "import isamcalc_lib as isam\n",
    "import subprocess\n",
    "import socplot_lib as socplt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#======================================================\n",
    "## Organize the data\n",
    "#======================================================\n",
    "totlen = 403248     # From 1995 to 2017\n",
    "totyrrange = 1995 + np.arange(23)\n",
    "day_of_year = [365, 366, 365, 365, 365, 366, 365, 365, 365, 366, 365, \\\n",
    "               365, 365, 366, 365, 365, 365, 366, 365, 365, 365, 366, 365]   # DOY from 1995 to 2017\n",
    "days_of_m = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "slices_of_year = np.dot(48,day_of_year)\n",
    "for i in np.arange(1,len(slices_of_year)):\n",
    "    slices_of_year[i] = slices_of_year[i] + slices_of_year[i-1]\n",
    "\n",
    "# Determine the data length\n",
    "pid = np.ones(totlen)\n",
    "idx = 0\n",
    "for i in np.arange(0,totlen):\n",
    "    idx = idx + 1\n",
    "    pid[i] = idx\n",
    "\n",
    "yr = np.ones(totlen)\n",
    "ptyear = 0\n",
    "for i in np.arange(0,totlen):\n",
    "    yr[i] = totyrrange[ptyear]\n",
    "    if (i >= slices_of_year[ptyear] - 1):\n",
    "        ptyear = ptyear + 1\n",
    "        \n",
    "doy = np.ones(totlen)\n",
    "hr = np.ones(totlen)\n",
    "accuday = 0\n",
    "for i in np.arange(0,len(day_of_year)):\n",
    "    ptday = 0\n",
    "    for j in np.arange(0,day_of_year[i]):\n",
    "        ptday = ptday + 1\n",
    "        for k in np.arange(0,48):\n",
    "            doy[k+accuday*48] = ptday\n",
    "            hr[k+accuday*48] = k/2. + 0.25\n",
    "        accuday = accuday + 1\n",
    "\n",
    "\n",
    "#======================================================\n",
    "## Read the observation of FCH4\n",
    "#======================================================\n",
    "varname1 = 'FCH4'\n",
    "varname2 = 'FCH4_CMB'\n",
    "\n",
    "# Aeriflux\n",
    "# Data are from different sources \n",
    "# so we extract observation from each site separately\n",
    "\n",
    "# US-PFa site data (AMF BASE product) \n",
    "# hrly dataset\n",
    "# Unit = nmol m-2 s-1\n",
    "filename = 'AMF_US-PFa_BASE_HR_10-4.csv'\n",
    "data = pd.read_csv(filename, encoding='iso-8859-1', skiprows=[0, 1]) #, index_col='ProfileID', skiprows=[1])\n",
    "yrrange = 1995 + np.arange(23)   # 23 years data\n",
    "res = 1   # Unit: hour\n",
    "CH4 = data.FCH4_1_1_1.as_matrix()\n",
    "obs_date = (data.TIMESTAMP_START.as_matrix() + data.TIMESTAMP_END.as_matrix())/2.\n",
    "totlen = 403248\n",
    "# Coordinate the time period of FCH4 timeseries for US-PFa\n",
    "ch4_uspfa = np.ones(totlen)\n",
    "for i in np.arange(0,len(CH4)):\n",
    "    for j in np.arange(0,2):\n",
    "        ch4_uspfa[2*i+j] = CH4[i]\n",
    "\n",
    "# US-WPT site data (OLD AMF data accessed from ORNL FTP site)\n",
    "# 30m dataset\n",
    "# Unit = umol m-2 s-1\n",
    "sitename = ['USWPT']\n",
    "ch4_uswpt = np.ones(totlen)*(-9999.)\n",
    "for i in sitename:\n",
    "    cmd = \"ls -1 AMF_\"+i+\"_*.csv\"\n",
    "    flist = subprocess.check_output(cmd, shell=True).splitlines()\n",
    "    yrrange=np.zeros(len(flist))\n",
    "    for j in np.arange(0,len(flist)):\n",
    "        yrrange[j] = int(flist[j][10:14])\n",
    "        data = pd.read_csv(flist[j], encoding='iso-8859-1')\n",
    "        CH4 = 0.001 * data.FCH4.as_matrix()   # convert to nmol\n",
    "        CH4[CH4 < -5.] = -9999.0\n",
    "        obs_date = data.TIMESTAMP.as_matrix() + 1500      \n",
    "        # Coordinate the time period of FCH4 timeseries for US-WPT\n",
    "        yr_site = np.floor(obs_date/1e10)\n",
    "        dd = np.floor((obs_date/1e10-np.floor(obs_date/1e10))*1e4)\n",
    "        hh = np.round((obs_date/1e6 -np.floor(obs_date/1e6))*1e2)\n",
    "        mm = np.round((obs_date/1e4 -np.floor(obs_date/1e4))*1e2)/60.\n",
    "        mo = np.floor(dd/1e2).astype(\"int\")\n",
    "        hr_site=hh+mm\n",
    "        # Get the first time step of the timeseries from file\n",
    "        sitelen = len(CH4)\n",
    "        pt = 0\n",
    "        yr_site_pt = yr_site[pt].astype(\"int\")\n",
    "        doy_site_pt = int((mo[pt]-1)*days_of_m[mo[pt]] + ((dd[pt]/1e2-np.floor(dd[pt]/1e2))*1e2))\n",
    "        hr_site_pt = hr_site[pt]\n",
    "        # Place into the array\n",
    "        for k in np.arange(0,totlen):\n",
    "            if(yr_site_pt == yr[k] and doy_site_pt == doy[k] and (hr_site_pt-hr[k])<1e-6):\n",
    "                idx = k\n",
    "                ch4_uswpt[idx:idx+sitelen] = CH4\n",
    "\n",
    "\n",
    "# US-ORv site data (OLD AMF data accessed from ORNL FTP site)\n",
    "# 30m dataset\n",
    "# Unit = umol m-2 s-1\n",
    "sitename = ['ORW']\n",
    "ch4_usorv = np.ones(totlen)*(-9999.)\n",
    "for i in sitename:\n",
    "    cmd = \"ls -1 \"+i+\"_*.csv\"\n",
    "    flist = subprocess.check_output(cmd, shell=True).splitlines()\n",
    "    yrrange=np.zeros(len(flist))\n",
    "    for j in np.arange(0,len(flist)):\n",
    "        yrrange[j] = int(flist[j][4:8])\n",
    "        data = pd.read_csv(flist[j], encoding='iso-8859-1', skiprows=[0, 1, 2, 4])\n",
    "        CH4 = 1000. * data.FCH4.as_matrix()      # convert to nmol\n",
    "        obs_date = 10000000 * data.YEAR.as_matrix() + 10000 * data.DOY.as_matrix() \\\n",
    "             + 100 * np.floor(data.HRMIN.as_matrix()) + 60 * (data.HRMIN.as_matrix() \\\n",
    "             - np.floor(data.HRMIN.as_matrix())) + 15\n",
    "        # Coordinate the time period of FCH4 timeseries for US-WPT\n",
    "        yr_site = np.floor(obs_date/1e7)\n",
    "        dd = np.floor((obs_date/1e7-np.floor(obs_date/1e7))*1e3)\n",
    "        hh = np.round((obs_date/1e4 -np.floor(obs_date/1e4))*1e2)\n",
    "        mm = np.round((obs_date/1e2 -np.floor(obs_date/1e2))*1e2)/60.\n",
    "        mo = np.floor(dd/1e2).astype(\"int\")\n",
    "        hr_site=hh+mm\n",
    "        # Get the first time step of the timeseries from file\n",
    "        sitelen = len(CH4)\n",
    "        pt = 0\n",
    "        yr_site_pt = yr_site[pt].astype(\"int\")\n",
    "        doy_site_pt = int(dd[pt])\n",
    "        hr_site_pt = hr_site[pt]\n",
    "        # Place into the array\n",
    "        for k in np.arange(0,totlen):\n",
    "            if(yr_site_pt == yr[k] and doy_site_pt == doy[k] and (hr_site_pt-hr[k])<1e-6):\n",
    "                idx = k\n",
    "                ch4_usorv[idx:idx+sitelen] = CH4\n",
    "            \n",
    "# US-Bes site data (OLD AMF data accessed from ORNL FTP site)\n",
    "# 30m dataset\n",
    "# Unit = umol m-2 s-1\n",
    "sitename = ['US-Bes']\n",
    "ch4_usbes = np.ones(totlen)*(-9999.)\n",
    "for i in sitename:\n",
    "    cmd = \"ls -1 \"+i+\"_*.csv\"\n",
    "    flist = subprocess.check_output(cmd, shell=True).splitlines()\n",
    "    yrrange=np.zeros(len(flist))\n",
    "    for j in np.arange(0,len(flist)):\n",
    "        yrrange[j] = int(flist[j][7:11])\n",
    "        data = pd.read_csv(flist[j], encoding='iso-8859-1', skiprows=[1])\n",
    "        CH4 = 1000. * data.FCH4.as_matrix()   # convert to nmol\n",
    "        obs_date = data.TIMESTAMP.as_matrix()\n",
    "        # Coordinate the time period of FCH4 timeseries for US-WPT\n",
    "        yr_site = np.floor(obs_date/1e8)\n",
    "        dd = np.floor((obs_date/1e8 -np.floor(obs_date/1e8))*1e4)\n",
    "        hh = np.round((obs_date/1e4 -np.floor(obs_date/1e4))*1e2)\n",
    "        mm = np.round((obs_date/1e2 -np.floor(obs_date/1e2))*1e2)/60.\n",
    "        mo = np.floor(dd/1e2).astype(\"int\")\n",
    "        hr_site=hh+mm\n",
    "        # Get the first time step of the timeseries from file\n",
    "        sitelen = len(CH4)\n",
    "        pt = 0\n",
    "        yr_site_pt = yr_site[pt].astype(\"int\")\n",
    "        doy_site_pt = int((mo[pt]-1)*days_of_m[mo[pt]] + ((dd[pt]/1e2-np.floor(dd[pt]/1e2))*1e2))\n",
    "        hr_site_pt = hr_site[pt]\n",
    "        # Place into the array\n",
    "        for k in np.arange(0,totlen):\n",
    "            if(yr_site_pt == yr[k] and doy_site_pt == doy[k] and (hr_site_pt-hr[k])<1e-6):\n",
    "                idx = k\n",
    "                ch4_usbes[idx:idx+sitelen] = CH4\n",
    "        \n",
    "        \n",
    "# US-IVO site data (OLD AMF data accessed from ORNL FTP site)\n",
    "# 30m dataset\n",
    "# Unit = umol m-2 s-1\n",
    "sitename = ['US-IVO']\n",
    "ch4_usivo = np.ones(totlen)*(-9999.)\n",
    "for i in sitename:\n",
    "    cmd = \"ls -1 \"+i+\"_*.csv\"\n",
    "    flist = subprocess.check_output(cmd, shell=True).splitlines()\n",
    "    yrrange=np.zeros(len(flist))\n",
    "    for j in np.arange(0,len(flist)):\n",
    "        yrrange[j] = int(flist[j][7:11])\n",
    "        data = pd.read_csv(flist[j], encoding='iso-8859-1', skiprows=[1])\n",
    "        CH4 = 1000. * data.FCH4.as_matrix()   # convert to nmol\n",
    "        obs_date = data.TIMESTAMP.as_matrix()\n",
    "        # Coordinate the time period of FCH4 timeseries for US-WPT\n",
    "        yr_site = np.floor(obs_date/1e8)\n",
    "        dd = np.floor((obs_date/1e8 -np.floor(obs_date/1e8))*1e4)\n",
    "        hh = np.round((obs_date/1e4 -np.floor(obs_date/1e4))*1e2)\n",
    "        mm = np.round((obs_date/1e2 -np.floor(obs_date/1e2))*1e2)/60.\n",
    "        mo = np.floor(dd/1e2).astype(\"int\")\n",
    "        hr_site=hh+mm\n",
    "        # Get the first time step of the timeseries from file\n",
    "        sitelen = len(CH4)\n",
    "        pt = 0\n",
    "        yr_site_pt = yr_site[pt].astype(\"int\")\n",
    "        doy_site_pt = int((mo[pt]-1)*days_of_m[mo[pt]] + ((dd[pt]/1e2-np.floor(dd[pt]/1e2))*1e2))\n",
    "        hr_site_pt = hr_site[pt]\n",
    "        # Place into the array\n",
    "        for k in np.arange(0,totlen):\n",
    "            if(yr_site_pt == yr[k] and doy_site_pt == doy[k] and (hr_site_pt-hr[k])<1e-6):\n",
    "                idx = k\n",
    "                ch4_usivo[idx:idx+sitelen] = CH4\n",
    "            \n",
    "# EFDC EC data\n",
    "# 30m dataset\n",
    "# Totally 4 sites are available\n",
    "# Unit = umol m-2 s-1\n",
    "sitename = ['DESfN', 'FIHyy', 'NLHor', 'RUCh2']\n",
    "for i in sitename:\n",
    "    if(i == 'DESfN'):\n",
    "        ch4_desfn = np.ones(totlen)*(-9999.)\n",
    "    if(i == 'FIHyy'):\n",
    "        ch4_fihyy = np.ones(totlen)*(-9999.)\n",
    "    if(i == 'NLHor'):\n",
    "        ch4_nlhor = np.ones(totlen)*(-9999.)\n",
    "    if(i == 'RUCh2'):\n",
    "        ch4_ruche = np.ones(totlen)*(-9999.)\n",
    "    cmd = \"ls -1 EFDC_*\"+i+\"_*30m.txt\"\n",
    "    flist = subprocess.check_output(cmd, shell=True).splitlines()\n",
    "    yrrange=np.zeros(len(flist))\n",
    "    for j in np.arange(0,len(flist)):\n",
    "        yrrange[j] = int(flist[j][18:22])\n",
    "        data = pd.read_csv(flist[j], encoding='iso-8859-1')\n",
    "        CH4 = data.FCH4.as_matrix()\n",
    "        timest = data.TIMESTAMP_START.as_matrix()\n",
    "        timeend = data.TIMESTAMP_END.as_matrix()\n",
    "        sel = ((timeend/1e2-np.floor(timeend/1e2))*1e2 == 0)\n",
    "        timeend[sel] = timeend[sel] - 40\n",
    "        sel = (np.round((timeend/1e4-np.floor(timeend/1e4))*1e4) == 9960)\n",
    "        timeend[sel] = timeend[sel] - 7600\n",
    "        obs_date = np.floor(timest/1e2)*1e2 + (timest/1e2-np.floor(timest/1e2) + \\\n",
    "                                               timeend/1e2-np.floor(timeend/1e2))*1e2/2.\n",
    "\n",
    "        # Coordinate the time period of FCH4 timeseries for US-WPT\n",
    "        yr_site = np.floor(obs_date/1e8)\n",
    "        dd = np.floor((obs_date/1e8 -np.floor(obs_date/1e8))*1e4)\n",
    "        hh = np.round((obs_date/1e4 -np.floor(obs_date/1e4))*1e2)\n",
    "        mm = np.round((obs_date/1e2 -np.floor(obs_date/1e2))*1e2)/60.\n",
    "        mo = np.floor(dd/1e2).astype(\"int\")\n",
    "        hr_site=hh+mm\n",
    "        # Get the first time step of the timeseries from file\n",
    "        sitelen = len(CH4)\n",
    "        pt = 0\n",
    "        yr_site_pt = yr_site[pt].astype(\"int\")\n",
    "        doy_site_pt = int((mo[pt]-1)*days_of_m[mo[pt]] + ((dd[pt]/1e2-np.floor(dd[pt]/1e2))*1e2))\n",
    "        hr_site_pt = hr_site[pt]\n",
    "        # Place into the array\n",
    "        for k in np.arange(0,totlen):\n",
    "            if(yr_site_pt == yr[k] and doy_site_pt == doy[k] and (hr_site_pt-hr[k])<1e-6):\n",
    "                idx = k\n",
    "                if(i == 'DESfN'):\n",
    "                    ch4_desfn[idx:idx+sitelen] = CH4\n",
    "                if(i == 'FIHyy'):\n",
    "                    ch4_fihyy[idx:idx+sitelen] = CH4\n",
    "                if(i == 'NLHor'):\n",
    "                    ch4_nlhor[idx:idx+sitelen] = CH4\n",
    "                if(i == 'RUCh2'):\n",
    "                    ch4_ruche[idx:idx+sitelen] = CH4\n",
    "                    \n",
    "# AU-RFi site data (Ridge Field, from Ozflux)\n",
    "# 30min dataset\n",
    "# Unit = umol m-2 s-1\n",
    "sitename = ['AU-RFi']\n",
    "varname = 'Fch4'\n",
    "ch4_aurfi = np.ones(totlen)*(-9999.)\n",
    "for i in sitename:\n",
    "    cmd = \"ls -1 Ridgefield_*.nc\"\n",
    "    flist = subprocess.check_output(cmd, shell=True).splitlines()\n",
    "    yrrange=np.zeros(len(flist))\n",
    "    for j in np.arange(0,len(flist)):\n",
    "        yrrange[j] = int(flist[j][11:15])\n",
    "        nc = Dataset(flist[j], 'r', format ='NETCDF4_CLASSIC')\n",
    "        yr_site = np.squeeze(nc.variables['Year'][:])  # extract/copy the data\n",
    "        dd = np.squeeze(nc.variables['Ddd'][:])\n",
    "        hh = np.squeeze(nc.variables['Hour'][:])\n",
    "        mm = np.squeeze(nc.variables['Minute'][:])\n",
    "        CH4 = 1000. * np.squeeze(nc.variables['Fch4'][:]) # shape is lat, lon as shown above\n",
    "        CH4[CH4 > 500.] = -9999.0\n",
    "        nc.close()\n",
    "        # Coordinate the time period of FCH4 timeseries for US-WPT\n",
    "        mm = mm/60.\n",
    "        hr_site=hh+mm\n",
    "        # Get the first time step of the timeseries from file\n",
    "        sitelen = len(CH4)\n",
    "        pt = 0\n",
    "        yr_site_pt = yr_site[pt].astype(\"int\")\n",
    "        doy_site_pt = dd[pt].astype(\"int\")\n",
    "        hr_site_pt = hr_site[pt]\n",
    "        # Place into the array\n",
    "        for k in np.arange(0,totlen):\n",
    "            if(yr_site_pt == yr[k] and doy_site_pt == doy[k] and (hr_site_pt-hr[k])<1e-6):\n",
    "                idx = k\n",
    "                ch4_aurfi[idx:idx+sitelen] = CH4\n",
    "                    \n",
    "# PH-IRI site data (Ridge Field, from Ozflux)\n",
    "# 30min dataset\n",
    "# Unit = umol m-2 s-1\n",
    "sitename = ['PH-IRI']\n",
    "varname = 'CH4'\n",
    "ch4_phiri = np.ones(totlen)*(-9999.)\n",
    "for i in sitename:\n",
    "    cmd = \"ls -1 FxMt_IRI-FL_*.csv\"\n",
    "    flist = subprocess.check_output(cmd, shell=True).splitlines()\n",
    "    yrrange=np.zeros(len(flist))\n",
    "    for j in np.arange(0,len(flist)):\n",
    "        yrrange[j] = int(flist[j][12:16])\n",
    "        data = pd.read_csv(flist[j], encoding='iso-8859-1', skiprows=[1])\n",
    "        CH4 = 1000. * data.CH4.as_matrix()   # convert to nmol\n",
    "        # Coordinate the time period of FCH4 timeseries for US-WPT\n",
    "        yr_site = data.Year.as_matrix()\n",
    "        dd = data.DOY.as_matrix()\n",
    "        hh = np.round(data.TIME.as_matrix()/1e2)\n",
    "        mm = np.round((data.TIME.as_matrix()/1e2 -np.floor(data.TIME.as_matrix()/1e2))*1e2)/60. - 0.25\n",
    "        hr_site=hh+mm\n",
    "        # Get the first time step of the timeseries from file\n",
    "        sitelen = len(CH4)\n",
    "        pt = 0\n",
    "        yr_site_pt = yr_site[pt].astype(\"int\")\n",
    "        doy_site_pt = dd[pt]\n",
    "        hr_site_pt = hr_site[pt]\n",
    "        # Place into the array\n",
    "        for k in np.arange(0,totlen):\n",
    "            if(yr_site_pt == yr[k] and doy_site_pt == doy[k] and (hr_site_pt-hr[k])<1e-6):\n",
    "                idx = k\n",
    "                ch4_phiri[idx:idx+sitelen] = CH4\n",
    "\n",
    "# Read in four old sites.\n",
    "# US-Twt\n",
    "# transfer from nmol m-2 s-1 to umol m-2 s-1\n",
    "sitename = ['US-Twt']\n",
    "varname = \"FCH4\"\n",
    "ch4_ustwt = np.ones(totlen)*(-9999.)\n",
    "fname = \"US-Twt_WT.nc\"\n",
    "nc = Dataset(fname, 'r', format ='NETCDF4_CLASSIC')\n",
    "yr_site = 2010  # extract/copy the data\n",
    "dd = 1\n",
    "hh = 0\n",
    "mm = 0.25\n",
    "CH4 = 0.001 * np.squeeze(nc.variables['FCH4'][:]) # shape is lat, lon as shown above\n",
    "# Do not filter the measurment from US-Twt site.\n",
    "#CH4[CH4 < -500.] = -9999.0   \n",
    "nc.close()\n",
    "hr_site=hh+mm\n",
    "# Get the first time step of the timeseries from file\n",
    "sitelen = len(CH4)\n",
    "yr_site_pt = yr_site\n",
    "doy_site_pt = dd\n",
    "hr_site_pt = hr_site\n",
    "# Place into the array\n",
    "for k in np.arange(0,totlen):\n",
    "    if(yr_site_pt == yr[k] and doy_site_pt == doy[k] and (hr_site_pt-hr[k])<1e-6):\n",
    "        idx = k\n",
    "        ch4_ustwt[idx:idx+sitelen] = CH4\n",
    "\n",
    "# US-Myb\n",
    "# transfer from nmol m-2 s-1 to umol m-2 s-1\n",
    "sitename = ['US-Myb']\n",
    "varname = \"FCH4\"\n",
    "ch4_usmyb = np.ones(totlen)*(-9999.)\n",
    "fname = \"US-Myb_WT.nc\"\n",
    "nc = Dataset(fname, 'r', format ='NETCDF4_CLASSIC')\n",
    "yr_site = 2011  # extract/copy the data\n",
    "dd = 1\n",
    "hh = 0\n",
    "mm = 0.25\n",
    "CH4 = 0.001 * np.squeeze(nc.variables['FCH4'][:]) # shape is lat, lon as shown above\n",
    "CH4[CH4 < -5.] = -9999.0\n",
    "nc.close()\n",
    "hr_site=hh+mm\n",
    "# Get the first time step of the timeseries from file\n",
    "sitelen = len(CH4)\n",
    "yr_site_pt = yr_site\n",
    "doy_site_pt = dd\n",
    "hr_site_pt = hr_site\n",
    "# Place into the array\n",
    "for k in np.arange(0,totlen):\n",
    "    if(yr_site_pt == yr[k] and doy_site_pt == doy[k] and (hr_site_pt-hr[k])<1e-6):\n",
    "        idx = k\n",
    "        ch4_usmyb[idx:idx+sitelen] = CH4\n",
    "\n",
    "# US-CRT\n",
    "# transfer from nmol m-2 s-1 to umol m-2 s-1\n",
    "sitename = ['US-CRT']\n",
    "varname = \"FCH4\"\n",
    "ch4_uscrt = np.ones(totlen)*(-9999.)\n",
    "fname = \"US-CRT_WT.nc\"\n",
    "nc = Dataset(fname, 'r', format ='NETCDF4_CLASSIC')\n",
    "yr_site = 2011  # extract/copy the data\n",
    "dd = 1\n",
    "hh = 0\n",
    "mm = 0.25\n",
    "CH4 = 0.001 * np.squeeze(nc.variables['FCH4'][:]) # shape is lat, lon as shown above\n",
    "CH4[CH4 < -5.] = -9999.0\n",
    "nc.close()\n",
    "hr_site=hh+mm\n",
    "# Get the first time step of the timeseries from file\n",
    "sitelen = len(CH4)\n",
    "yr_site_pt = yr_site\n",
    "doy_site_pt = dd\n",
    "hr_site_pt = hr_site\n",
    "# Place into the array\n",
    "for k in np.arange(0,totlen):\n",
    "    if(yr_site_pt == yr[k] and doy_site_pt == doy[k] and (hr_site_pt-hr[k])<1e-6):\n",
    "        idx = k\n",
    "        ch4_uscrt[idx:idx+sitelen] = CH4\n",
    "        \n",
    "# US-Tw1\n",
    "# transfer from nmol m-2 s-1 to umol m-2 s-1\n",
    "sitename = ['US-Tw1']\n",
    "varname = \"FCH4\"\n",
    "ch4_ustw1 = np.ones(totlen)*(-9999.)\n",
    "fname = \"US-Tw1_WT.nc\"\n",
    "nc = Dataset(fname, 'r', format ='NETCDF4_CLASSIC')\n",
    "yr_site = 2013  # extract/copy the data\n",
    "dd = 1\n",
    "hh = 0\n",
    "mm = 0.25\n",
    "CH4 = 0.001 * np.squeeze(nc.variables['FCH4'][:]) # shape is lat, lon as shown above\n",
    "CH4[CH4 < -5.] = -9999.0\n",
    "nc.close()\n",
    "hr_site=hh+mm\n",
    "# Get the first time step of the timeseries from file\n",
    "sitelen = len(CH4)\n",
    "yr_site_pt = yr_site\n",
    "doy_site_pt = dd\n",
    "hr_site_pt = hr_site\n",
    "# Place into the array\n",
    "for k in np.arange(0,totlen):\n",
    "    if(yr_site_pt == yr[k] and doy_site_pt == doy[k] and (hr_site_pt-hr[k])<1e-6):\n",
    "        idx = k\n",
    "        ch4_ustw1[idx:idx+sitelen] = CH4\n",
    "        \n",
    "# Create dataframe to combine all observations\n",
    "d = {'ID': pid, 'YEAR': yr, 'DOY': doy, 'TIME': hr, 'USPFa': ch4_uspfa, 'USWPT': ch4_uswpt, \\\n",
    "     'USORv': ch4_usorv, 'USBes': ch4_usbes, 'USIVO': ch4_usivo, 'DESfN': ch4_desfn, \\\n",
    "     'FIHyy': ch4_fihyy, 'NLHor': ch4_nlhor, 'RUCh2': ch4_ruche, 'AURFi': ch4_aurfi, \\\n",
    "     'PHIRI': ch4_phiri, 'USTwt': ch4_ustwt, 'USMyb': ch4_usmyb, 'USCRT': ch4_uscrt, \\\n",
    "     'USTw1': ch4_ustw1}\n",
    "\n",
    "methane = pd.DataFrame(data=d)\n",
    "# Write the data as CSV file\n",
    "methane.to_csv('site_methane.csv')\n",
    "\n",
    "# ===============================================\n",
    "## Checkout chamber data later\n",
    "# ===============================================\n",
    "# EFDC noon-time CH4 Chamber measurement\n",
    "# daily dataset\n",
    "# Totally 6 sites are available\n",
    "# Unit?\n",
    "if(0):   # Right now commented\n",
    "    sitename = ['FI-Hyy', 'FI-Kaa', 'IT-CA1', 'IT-CA2', 'IT-CA3', 'IT-Ro2']\n",
    "    for i in sitename:\n",
    "        cmd = \"ls -1 \"+i+\"_Fluxes*.txt\"\n",
    "        flist = subprocess.check_output(cmd, shell=True).splitlines()\n",
    "        yrrange=np.zeros(len(flist))\n",
    "        for j in np.arange(0,len(flist)):\n",
    "            yrrange[j] = int(flist[j][14:18])\n",
    "            data = pd.read_csv(flist[j], encoding='iso-8859-1')\n",
    "            #CH4 = data.FCH4.as_matrix()\n",
    "            #obs_date = (data.TIMESTAMP_START.as_matrix() + data.TIMESTAMP_END.as_matrix())/2.\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#======================================================\n",
    "## Organize the data\n",
    "#======================================================\n",
    "totlen = 403248     # From 1995 to 2017\n",
    "totyrrange = 1995 + np.arange(23)\n",
    "day_of_year = [365, 366, 365, 365, 365, 366, 365, 365, 365, 366, 365, \\\n",
    "               365, 365, 366, 365, 365, 365, 366, 365, 365, 365, 366, 365]   # DOY from 1995 to 2017\n",
    "days_of_m = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "slices_of_year = np.dot(48,day_of_year)\n",
    "for i in np.arange(1,len(slices_of_year)):\n",
    "    slices_of_year[i] = slices_of_year[i] + slices_of_year[i-1]\n",
    "\n",
    "# Determine the data length\n",
    "pid = np.ones(totlen)\n",
    "idx = 0\n",
    "for i in np.arange(0,totlen):\n",
    "    idx = idx + 1\n",
    "    pid[i] = idx\n",
    "\n",
    "yr = np.ones(totlen)\n",
    "ptyear = 0\n",
    "for i in np.arange(0,totlen):\n",
    "    yr[i] = totyrrange[ptyear]\n",
    "    if (i >= slices_of_year[ptyear] - 1):\n",
    "        ptyear = ptyear + 1\n",
    "        \n",
    "doy = np.ones(totlen)\n",
    "hr = np.ones(totlen)\n",
    "accuday = 0\n",
    "for i in np.arange(0,len(day_of_year)):\n",
    "    ptday = 0\n",
    "    for j in np.arange(0,day_of_year[i]):\n",
    "        ptday = ptday + 1\n",
    "        for k in np.arange(0,48):\n",
    "            doy[k+accuday*48] = ptday\n",
    "            hr[k+accuday*48] = k/2. + 0.25\n",
    "        accuday = accuday + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "methane.to_csv('site_methane.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt( \"sss\", ch4_ustwt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#======================================================\n",
    "## Organize the data\n",
    "#======================================================\n",
    "totlen = 403248     # From 1995 to 2017\n",
    "totyrrange = 1995 + np.arange(23)\n",
    "day_of_year = [365, 366, 365, 365, 365, 366, 365, 365, 365, 366, 365, \\\n",
    "               365, 365, 366, 365, 365, 365, 366, 365, 365, 365, 366, 365]   # DOY from 1995 to 2017\n",
    "days_of_m = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "slices_of_year = np.dot(48,day_of_year)\n",
    "for i in np.arange(1,len(slices_of_year)):\n",
    "    slices_of_year[i] = slices_of_year[i] + slices_of_year[i-1]\n",
    "\n",
    "\n",
    "# Site info: lat, lon and data availability\n",
    "uspfa_loc = [45.9459, -90.2723]\n",
    "uspfa_yr  = [2010, 2014]\n",
    "uswpt_loc = [41.464639, -82.996157]\n",
    "uswpt_yr  = [2011, 2013]\n",
    "usorv_loc = [40.0201, -83.0183]\n",
    "usorv_yr  = [2011, 2012]\n",
    "usbes_loc = [71.2809, -156.5965]\n",
    "usbes_yr  = [2013, 2014]\n",
    "usivo_loc = [68.4865, -155.7503]\n",
    "usivo_yr  = [2013, 2014]\n",
    "desfn_loc = [47.8064, 11.3275]\n",
    "desfn_yr  = [2012, 2015]\n",
    "fihyy_loc = [61.8474, 24.2948]\n",
    "fihyy_yr  = [2012, 2013]\n",
    "nlhor_loc = [52.24035, 5.071301]\n",
    "nlhor_yr  = [2006, 2009]\n",
    "ruche_loc = [68.61304, 161.34143]\n",
    "ruche_yr  = [2014, 2015]\n",
    "aurfi_loc = [-32.5061, 116.9668]\n",
    "aurfi_yr  = [2016, 2017]\n",
    "phiri_loc = [14.1412, 121.2653]\n",
    "phiri_yr  = [2013, 2014]\n",
    "\n",
    "\n",
    "methane = pd.read_csv('site_methane.csv')\n",
    "\n",
    "# Read in the model output and extract the corresponding grid point to compare with the observation.\n",
    "#fncname = 'Global_1DSBGC.bgc-yearly-2d_1920.nc'\n",
    "#nc = Dataset(fncname, 'r', format ='NETCDF4_CLASSIC')\n",
    "#lats = nc.variables['lat'][:]  # extract/copy the data\n",
    "#lons = nc.variables['lon'][:]\n",
    "#ch4_isam = nc.variables['ch4_flux'][:] # shape is lat, lon as shown above\n",
    "#nc.close()\n",
    "\n",
    "# Read site level simulation resylts\n",
    "reader = pd.read_csv('uspfa_ch4.txt')\n",
    "\n",
    "uspfa_isam_yr = [uspfa_yr[0] - 1990, uspfa_yr[1] - 1990]\n",
    "uspfa_isam = reader.FCH4.as_matrix()\n",
    "uspfa_isam = uspfa_isam[uspfa_isam_yr[0]*365:(uspfa_isam_yr[1])*365]\n",
    "uspfa_isam = uspfa_isam*1e9/(12*24*3600)\n",
    "# Calculate daily mean\n",
    "uspfa_obs_yr = [uspfa_yr[0] - 1995, uspfa_yr[1] - 1995]\n",
    "uspfa_obs = methane.USPFa[slices_of_year[uspfa_obs_yr[0]]:slices_of_year[uspfa_obs_yr[1]]]\n",
    "uspfa_obs[uspfa_obs<-100] = np.float(\"nan\")\n",
    "uspfa_obs_daily = np.ones(len(uspfa_isam))*float(\"nan\")\n",
    "for j in np.arange(0,(uspfa_obs_yr[1]-uspfa_obs_yr[0])):\n",
    "    for i in np.arange(0,365):\n",
    "        uspfa_obs_daily[365*j+i] = np.nanmean(uspfa_obs[j*17520+i*48:j*17520+(i+1)*48])\n",
    "\n",
    "# ========================================================\n",
    "# Plot the modeled CH4 fluxes against EC observation\n",
    "# ========================================================\n",
    "Xobs = np.arange(0,len(uspfa_obs_daily))\n",
    "Yobs = uspfa_obs_daily\n",
    "Xmod = Xobs\n",
    "Ymod = uspfa_isam\n",
    "tit = \"CH4 flux\"\n",
    "path = \"./uspfa.jpg\"\n",
    "\n",
    "status = socplt.plot_obsvsmod(Xobs, Yobs, Xmod, Ymod, tit, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read site level simulation resylts\n",
    "reader = pd.read_csv('usbes_ch4.txt')\n",
    "\n",
    "usbes_isam_yr = [usbes_yr[0] - 1990, usbes_yr[1] - 1990]\n",
    "usbes_isam = reader.FCH4.as_matrix()\n",
    "usbes_isam = usbes_isam[usbes_isam_yr[0]*365:(usbes_isam_yr[1])*365]\n",
    "usbes_isam = usbes_isam*1e9/(12*24*3600)\n",
    "# Calculate daily mean\n",
    "usbes_obs_yr = [usbes_yr[0] - 1995, usbes_yr[1] - 1995]\n",
    "usbes_obs = methane.USBes[slices_of_year[usbes_obs_yr[0]]:slices_of_year[usbes_obs_yr[1]]]\n",
    "usbes_obs[usbes_obs<-100] = np.float(\"nan\")\n",
    "usbes_obs_daily = np.ones(len(usbes_isam))*float(\"nan\")\n",
    "for j in np.arange(0,(usbes_obs_yr[1]-usbes_obs_yr[0])):\n",
    "    for i in np.arange(0,365):\n",
    "        usbes_obs_daily[365*j+i] = np.nanmean(usbes_obs[j*17520+i*48:j*17520+(i+1)*48])\n",
    "\n",
    "# ========================================================\n",
    "# Plot the modeled CH4 fluxes against EC observation\n",
    "# ========================================================\n",
    "Xobs = np.arange(0,len(usbes_obs_daily))\n",
    "Yobs = usbes_obs_daily/10.\n",
    "Xmod = Xobs\n",
    "Ymod = usbes_isam\n",
    "tit = \"CH4 flux\"\n",
    "path = \"./usbes.jpg\"\n",
    "\n",
    "status = socplt.plot_obsvsmod(Xobs, Yobs, Xmod, Ymod, tit, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read site level simulation resylts\n",
    "reader = pd.read_csv('usivo_ch4.txt')\n",
    "\n",
    "usivo_isam_yr = [usivo_yr[0] - 1990, usivo_yr[1] - 1990]\n",
    "usivo_isam = reader.FCH4.as_matrix()\n",
    "usivo_isam = usivo_isam[usivo_isam_yr[0]*365:(usivo_isam_yr[1])*365]\n",
    "usivo_isam = usivo_isam*1e9/(12*24*3600)\n",
    "# Calculate daily mean\n",
    "usivo_obs_yr = [usivo_yr[0] - 1995, usivo_yr[1] - 1995]\n",
    "usivo_obs = methane.USIVO[slices_of_year[usivo_obs_yr[0]]:slices_of_year[usivo_obs_yr[1]]]\n",
    "usivo_obs[usivo_obs<-100] = np.float(\"nan\")\n",
    "usivo_obs_daily = np.ones(len(usivo_isam))*float(\"nan\")\n",
    "for j in np.arange(0,(usivo_obs_yr[1]-usivo_obs_yr[0])):\n",
    "    for i in np.arange(0,365):\n",
    "        usivo_obs_daily[365*j+i] = np.nanmean(usivo_obs[j*17520+i*48:j*17520+(i+1)*48])\n",
    "\n",
    "# ========================================================\n",
    "# Plot the modeled CH4 fluxes against EC observation\n",
    "# ========================================================\n",
    "Xobs = np.arange(0,len(usivo_obs_daily))\n",
    "Yobs = usivo_obs_daily/10.\n",
    "Xmod = Xobs\n",
    "Ymod = usivo_isam\n",
    "tit = \"CH4 flux\"\n",
    "path = \"./usivo.jpg\"\n",
    "\n",
    "status = socplt.plot_obsvsmod(Xobs, Yobs, Xmod, Ymod, tit, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read site level simulation resylts\n",
    "reader = pd.read_csv('uswpt_ch4.txt')\n",
    "\n",
    "uswpt_isam_yr = [uswpt_yr[0] - 1990, uswpt_yr[1] - 1990]\n",
    "uswpt_isam = reader.FCH4.as_matrix()\n",
    "uswpt_isam = uswpt_isam[uswpt_isam_yr[0]*365:(uswpt_isam_yr[1])*365]\n",
    "uswpt_isam = uswpt_isam*1e9/(12*24*3600)\n",
    "# Calculate daily mean\n",
    "uswpt_obs_yr = [uswpt_yr[0] - 1995, uswpt_yr[1] - 1995]\n",
    "uswpt_obs = methane.USWPT[slices_of_year[uswpt_obs_yr[0]]:slices_of_year[uswpt_obs_yr[1]]]\n",
    "uswpt_obs[uswpt_obs<-100] = np.float(\"nan\")\n",
    "uswpt_obs_daily = np.ones(len(uswpt_isam))*float(\"nan\")\n",
    "for j in np.arange(0,(uswpt_obs_yr[1]-uswpt_obs_yr[0])):\n",
    "    for i in np.arange(0,365):\n",
    "        uswpt_obs_daily[365*j+i] = np.nanmean(uswpt_obs[j*17520+i*48:j*17520+(i+1)*48])\n",
    "\n",
    "# ========================================================\n",
    "# Plot the modeled CH4 fluxes against EC observation\n",
    "# ========================================================\n",
    "Xobs = np.arange(0,len(uswpt_obs_daily))\n",
    "Yobs = uswpt_obs_daily*2/10000.\n",
    "Xmod = Xobs\n",
    "Ymod = uswpt_isam\n",
    "tit = \"CH4 flux\"\n",
    "path = \"./uswpt.jpg\"\n",
    "\n",
    "status = socplt.plot_obsvsmod(Xobs, Yobs, Xmod, Ymod, tit, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read site level simulation resylts\n",
    "reader = pd.read_csv('usorv_ch4.txt')\n",
    "\n",
    "usorv_isam_yr = [usorv_yr[0] - 1990, usorv_yr[1] - 1990]\n",
    "usorv_isam = reader.FCH4.as_matrix()\n",
    "usorv_isam = usorv_isam[usorv_isam_yr[0]*365:(usorv_isam_yr[1])*365]\n",
    "usorv_isam = usorv_isam*1e9/(12*24*3600)\n",
    "# Calculate daily mean\n",
    "usorv_obs_yr = [usorv_yr[0] - 1995, usorv_yr[1] - 1995]\n",
    "usorv_obs = methane.USORv[slices_of_year[usorv_obs_yr[0]]:slices_of_year[usorv_obs_yr[1]]]\n",
    "usorv_obs[usorv_obs<-100] = np.float(\"nan\")\n",
    "usorv_obs_daily = np.ones(len(usorv_isam))*float(\"nan\")\n",
    "for j in np.arange(0,(usorv_obs_yr[1]-usorv_obs_yr[0])):\n",
    "    for i in np.arange(0,365):\n",
    "        usorv_obs_daily[365*j+i] = np.nanmean(usorv_obs[j*17520+i*48:j*17520+(i+1)*48])\n",
    "\n",
    "# ========================================================\n",
    "# Plot the modeled CH4 fluxes against EC observation\n",
    "# ========================================================\n",
    "Xobs = np.arange(0,len(usorv_obs_daily))\n",
    "Yobs = usorv_obs_daily/10.\n",
    "Xmod = Xobs\n",
    "Ymod = usorv_isam\n",
    "tit = \"CH4 flux\"\n",
    "path = \"./usorv.jpg\"\n",
    "\n",
    "status = socplt.plot_obsvsmod(Xobs, Yobs, Xmod, Ymod, tit, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read site level simulation resylts\n",
    "reader = pd.read_csv('desfn_ch4.txt')\n",
    "\n",
    "desfn_isam_yr = [desfn_yr[0] - 1990, desfn_yr[1] - 1990]\n",
    "desfn_isam = reader.FCH4.as_matrix()\n",
    "desfn_isam = desfn_isam[desfn_isam_yr[0]*365:(desfn_isam_yr[1])*365]\n",
    "desfn_isam = desfn_isam*1e9/(12*24*3600)\n",
    "# Calculate daily mean\n",
    "desfn_obs_yr = [desfn_yr[0] - 1995, desfn_yr[1] - 1995]\n",
    "desfn_obs = methane.DESfN[slices_of_year[desfn_obs_yr[0]]:slices_of_year[desfn_obs_yr[1]]]\n",
    "desfn_obs[desfn_obs<-100] = np.float(\"nan\")\n",
    "desfn_obs_daily = np.ones(len(desfn_isam))*float(\"nan\")\n",
    "for j in np.arange(0,(desfn_obs_yr[1]-desfn_obs_yr[0])):\n",
    "    for i in np.arange(0,365):\n",
    "        desfn_obs_daily[365*j+i] = np.nanmean(desfn_obs[j*17520+i*48:j*17520+(i+1)*48])\n",
    "\n",
    "# ========================================================\n",
    "# Plot the modeled CH4 fluxes against EC observation\n",
    "# ========================================================\n",
    "Xobs = np.arange(0,len(desfn_obs_daily))\n",
    "Yobs = desfn_obs_daily*100.\n",
    "Xmod = Xobs\n",
    "Ymod = desfn_isam\n",
    "tit = \"CH4 flux\"\n",
    "path = \"./desfn.jpg\"\n",
    "\n",
    "status = socplt.plot_obsvsmod(Xobs, Yobs, Xmod, Ymod, tit, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read site level simulation resylts\n",
    "reader = pd.read_csv('fihyy_ch4.txt')\n",
    "\n",
    "fihyy_isam_yr = [fihyy_yr[0] - 1990, fihyy_yr[1] - 1990]\n",
    "fihyy_isam = reader.FCH4.as_matrix()\n",
    "fihyy_isam = fihyy_isam[fihyy_isam_yr[0]*365:(fihyy_isam_yr[1])*365]\n",
    "fihyy_isam = fihyy_isam*1e9/(12*24*3600)\n",
    "# Calculate daily mean\n",
    "fihyy_obs_yr = [fihyy_yr[0] - 1995, fihyy_yr[1] - 1995]\n",
    "fihyy_obs = methane.FIHyy[slices_of_year[fihyy_obs_yr[0]]:slices_of_year[fihyy_obs_yr[1]]]\n",
    "fihyy_obs[fihyy_obs<-100] = np.float(\"nan\")\n",
    "fihyy_obs_daily = np.ones(len(fihyy_isam))*float(\"nan\")\n",
    "for j in np.arange(0,(fihyy_obs_yr[1]-fihyy_obs_yr[0])):\n",
    "    for i in np.arange(0,365):\n",
    "        fihyy_obs_daily[365*j+i] = np.nanmean(fihyy_obs[j*17520+i*48:j*17520+(i+1)*48])\n",
    "\n",
    "# ========================================================\n",
    "# Plot the modeled CH4 fluxes against EC observation\n",
    "# ========================================================\n",
    "Xobs = np.arange(0,len(fihyy_obs_daily))\n",
    "Yobs = fihyy_obs_daily\n",
    "Xmod = Xobs\n",
    "Ymod = fihyy_isam\n",
    "tit = \"CH4 flux\"\n",
    "path = \"./fihyy.jpg\"\n",
    "\n",
    "status = socplt.plot_obsvsmod(Xobs, Yobs, Xmod, Ymod, tit, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read site level simulation resylts\n",
    "reader = pd.read_csv('nlhor_ch4.txt')\n",
    "\n",
    "nlhor_isam_yr = [nlhor_yr[0] - 1990, nlhor_yr[1] - 1990]\n",
    "nlhor_isam = reader.FCH4.as_matrix()\n",
    "nlhor_isam = nlhor_isam[nlhor_isam_yr[0]*365:(nlhor_isam_yr[1])*365]\n",
    "nlhor_isam = nlhor_isam*1e9/(12*24*3600)\n",
    "# Calculate daily mean\n",
    "nlhor_obs_yr = [nlhor_yr[0] - 1995, nlhor_yr[1] - 1995]\n",
    "nlhor_obs = methane.NLHor[slices_of_year[nlhor_obs_yr[0]]:slices_of_year[nlhor_obs_yr[1]]]\n",
    "nlhor_obs[nlhor_obs<-100] = np.float(\"nan\")\n",
    "nlhor_obs_daily = np.ones(len(nlhor_isam))*float(\"nan\")\n",
    "for j in np.arange(0,(nlhor_obs_yr[1]-nlhor_obs_yr[0])):\n",
    "    for i in np.arange(0,365):\n",
    "        nlhor_obs_daily[365*j+i] = np.nanmean(nlhor_obs[j*17520+i*48:j*17520+(i+1)*48])\n",
    "\n",
    "# ========================================================\n",
    "# Plot the modeled CH4 fluxes against EC observation\n",
    "# ========================================================\n",
    "Xobs = np.arange(0,len(nlhor_obs_daily))\n",
    "Yobs = nlhor_obs_daily*1000.\n",
    "Xmod = Xobs\n",
    "Ymod = nlhor_isam\n",
    "tit = \"CH4 flux\"\n",
    "path = \"./nlhor.jpg\"\n",
    "\n",
    "status = socplt.plot_obsvsmod(Xobs, Yobs, Xmod, Ymod, tit, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read site level simulation resylts\n",
    "reader = pd.read_csv('ruch2_ch4.txt')\n",
    "\n",
    "ruche_isam_yr = [ruche_yr[0] - 1990, ruche_yr[1] - 1990]\n",
    "ruche_isam = reader.FCH4.as_matrix()\n",
    "ruche_isam = ruche_isam[ruche_isam_yr[0]*365:(ruche_isam_yr[1])*365]\n",
    "ruche_isam = ruche_isam*1e9/(12*24*3600)\n",
    "# Calculate daily mean\n",
    "ruche_obs_yr = [ruche_yr[0] - 1995, ruche_yr[1] - 1995]\n",
    "ruche_obs = methane.RUChe[slices_of_year[ruche_obs_yr[0]]:slices_of_year[ruche_obs_yr[1]]]\n",
    "ruche_obs[ruche_obs<-100] = np.float(\"nan\")\n",
    "ruche_obs_daily = np.ones(len(ruche_isam))*float(\"nan\")\n",
    "for j in np.arange(0,(ruche_obs_yr[1]-ruche_obs_yr[0])):\n",
    "    for i in np.arange(0,365):\n",
    "        ruche_obs_daily[365*j+i] = np.nanmean(ruche_obs[j*17520+i*48:j*17520+(i+1)*48])\n",
    "\n",
    "# ========================================================\n",
    "# Plot the modeled CH4 fluxes against EC observation\n",
    "# ========================================================\n",
    "Xobs = np.arange(0,len(ruche_obs_daily))\n",
    "Yobs = ruche_obs_daily\n",
    "Xmod = Xobs\n",
    "Ymod = ruche_isam\n",
    "tit = \"CH4 flux\"\n",
    "path = \"./ruche.jpg\"\n",
    "\n",
    "status = socplt.plot_obsvsmod(Xobs, Yobs, Xmod, Ymod, tit, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ruche_yr  = [2014, 2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Plot the modeled CH4 fluxes against EC observation\n",
    "# ========================================================\n",
    "Xobs = np.arange(0,len(uspfa_obs_daily))\n",
    "Yobs = uspfa_obs_daily\n",
    "Xmod = Xobs\n",
    "Ymod = uspfa_isam\n",
    "tit = \"CH4 flux\"\n",
    "path = \"./uspfa.jpg\"\n",
    "\n",
    "status = socplt.plot_obsvsmod(Xobs, Yobs, Xmod, Ymod, tit, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "usorv_yr  = [2011, 2012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Plot the modeled CH4 fluxes against EC observation\n",
    "# ========================================================\n",
    "Xobs = np.arange(0,totlen)\n",
    "Yobs = methane.USPFa\n",
    "Yobs[Yobs < -1000.] = float('nan')\n",
    "Xmod = np.arange(0,totlen)\n",
    "Ymod = uspfa_isam * np.ones(totlen)\n",
    "tit = \"CH4 flux\"\n",
    "path = \"./uspfa.jpg\"\n",
    "\n",
    "status = socplt.plot_obsvsmod(Xobs, Yobs, Xmod, Ymod, tit, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Plot the site location on the map\n",
    "# ========================================================\n",
    "# Site info: lat and lon\n",
    "usmyb_loc = [38.0498, -121.7651]\n",
    "ustwt_loc = [38.10867, -121.6530]\n",
    "uscrt_loc = [41.6285, -83.3471]\n",
    "uspfa_loc = [45.9459, -90.2723]\n",
    "uswpt_loc = [41.464639, -82.996157]\n",
    "usorv_loc = [40.0201, -83.0183]\n",
    "usbes_loc = [71.2809, -156.5965]\n",
    "usivo_loc = [68.4865, -155.7503]\n",
    "desfn_loc = [47.8064, 11.3275]\n",
    "fihyy_loc = [61.8474, 24.2948]\n",
    "nlhor_loc = [52.24035, 5.071301]\n",
    "ruche_loc = [68.61304, 161.34143]\n",
    "aurfi_loc = [-32.5061, 116.9668]\n",
    "\n",
    "#%% plot all profiles\n",
    "#lons = [uspfa_loc[1], uswpt_loc[1], usorv_loc[1], usbes_loc[1], usivo_loc[1], desfn_loc[1], \\\n",
    "#        fihyy_loc[1], nlhor_loc[1], ruche_loc[1], aurfi_loc[1]]\n",
    "lons = [usmyb_loc[1], ustwt_loc[1], desfn_loc[1], \\\n",
    "        nlhor_loc[1]]\n",
    "#lats = [uspfa_loc[0], uswpt_loc[0], usorv_loc[0], usbes_loc[0], usivo_loc[0], desfn_loc[0], \\\n",
    "#        fihyy_loc[0], nlhor_loc[0], ruche_loc[0], aurfi_loc[0]]\n",
    "lats = [usmyb_loc[0], ustwt_loc[0], desfn_loc[0], \\\n",
    "        nlhor_loc[0]]\n",
    "fig = plt.figure(figsize=(24,12))\n",
    "ax = fig.add_axes([0.05,0.05,0.9,0.9])\n",
    "m = Basemap(llcrnrlon=-180,llcrnrlat=-60,urcrnrlon=180,urcrnrlat=80,resolution='l',projection='mill',lon_0=0,lat_0=0)\n",
    "lon, lat = np.meshgrid(lons, lats)\n",
    "X, Y = m(lon,lat)\n",
    "m.drawcoastlines(linewidth=0.25)\n",
    "m.drawcountries(linewidth=0.25)\n",
    "m.drawmapboundary(fill_color='#99ffff')\n",
    "m.fillcontinents(color='grey',lake_color='#99ffff',zorder=0)\n",
    "m.scatter(lons,lats,2000,marker='^',color='r',alpha=0.7,latlon=True)\n",
    "# draw parallels.\n",
    "parallels = np.arange(-90.,90.,30.)\n",
    "m.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)\n",
    "# draw meridians\n",
    "meridians = np.arange(0.,360.,45.)\n",
    "m.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)\n",
    "# put text\n",
    "xy = np.column_stack((X[0,:],Y[:,0]))\n",
    "#xy1 = xy[0:2,:]\n",
    "#xy2 = xy[2,:]\n",
    "#xy3 = xy[3:10,:]\n",
    "#text1 = ['US_PFa', 'US_WPT']\n",
    "#text2 = ['US_ORv']\n",
    "#text3 = ['US_Bes','US_IVO', 'DE_SfN', 'FI_Hyy', 'NL_Hor', 'RU_Ch2', 'AU_RFi']\n",
    "\n",
    "xy1 = xy[0:2,:]\n",
    "xy2 = xy[2,:]\n",
    "xy3 = xy[3:10,:]\n",
    "text1 = [' ', ' ']\n",
    "text2 = [' ']\n",
    "text3 = [' ', ' ']\n",
    "\n",
    "\n",
    "for label, (x,y) in zip(text1, xy1):\n",
    "    #ax.annotate(label, (x,y), xytext=(5, 5), textcoords='offset points')\n",
    "    ax.text(x, y, label, ha='left', size=30)\n",
    "ax.text(xy2[0], xy2[1], text2[0], ha='right', size=30)\n",
    "for label, (x,y) in zip(text3, xy3):\n",
    "    #ax.annotate(label, (x,y), xytext=(5, 5), textcoords='offset points')\n",
    "    ax.text(x, y, label, ha='left', size=30)\n",
    "    \n",
    "ax.set_title('CH4 flux EC measurement sites')\n",
    "plt.show()\n",
    "#fig.savefig('./methane_sitemap.png')\n",
    "    \n",
    "\n",
    "loc = isam.latlon_2_idx(uspfa_loc[0], uspfa_loc[1])\n",
    "uspfa_isam = ch4_isam[loc[0]-1, loc[1]-1]*1e9/(12*3600*365)\n",
    "loc = isam.latlon_2_idx(uswpt_loc[0], uswpt_loc[1])\n",
    "uswpt_isam = ch4_isam[loc[0]-1, loc[1]-1]*1e9/(12*3600*365)\n",
    "loc = isam.latlon_2_idx(usorv_loc[0], usorv_loc[1])\n",
    "usorv_isam = ch4_isam[loc[0]-1, loc[1]-1]*1e9/(12*3600*365)\n",
    "loc = isam.latlon_2_idx(usbes_loc[0], usbes_loc[1])\n",
    "usbes_isam = ch4_isam[loc[0]-1, loc[1]-1]*1e9/(12*3600*365)\n",
    "loc = isam.latlon_2_idx(usivo_loc[0], usivo_loc[1])\n",
    "usivo_isam = ch4_isam[loc[0]-1, loc[1]-1]*1e9/(12*3600*365)\n",
    "loc = isam.latlon_2_idx(desfn_loc[0], desfn_loc[1])\n",
    "desfn_isam = ch4_isam[loc[0]-1, loc[1]-1]*1e9/(12*3600*365)\n",
    "loc = isam.latlon_2_idx(fihyy_loc[0], fihyy_loc[1])\n",
    "fihyy_isam = ch4_isam[loc[0]-1, loc[1]-1]*1e9/(12*3600*365)\n",
    "loc = isam.latlon_2_idx(nlhor_loc[0], nlhor_loc[1])\n",
    "nlhor_isam = ch4_isam[loc[0]-1, loc[1]-1]*1e9/(12*3600*365)\n",
    "loc = isam.latlon_2_idx(ruche_loc[0], ruche_loc[1])\n",
    "ruche_isam = ch4_isam[loc[0]-1, loc[1]-1]*1e9/(12*3600*365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_profid = data.index.unique()\n",
    "lons = prep.getvarxls(data,'Lon',all_profid[0:16],0)\n",
    "lats = prep.getvarxls(data,'Lat',all_profid[0:16],0)\n",
    "\n",
    "# Read in model output\n",
    "d14cm = pd.read_table('isam_dc14.dat', header=None, delimiter=r\"\\s+\")\n",
    "# Read in another case from the ISAM model output\n",
    "# d14cm2 = pd.read_table('isam_dc14.dat', header=None, delimiter=r\"\\s+\")\n",
    "d14cm.columns = ['ID', 'Layer1', 'Layer2', 'Layer3', 'Layer4', 'Layer5', 'Layer6', 'Layer7', 'Layer8', 'Layer9', 'Layer10']\n",
    "d14cm = d14cm.set_index('ID')\n",
    "mod_profid = d14cm.index\n",
    "# d14cm2.columns = ['ID', 'Layer1', 'Layer2', 'Layer3', 'Layer4', 'Layer5', 'Layer6', 'Layer7', 'Layer8', 'Layer9', 'Layer10']\n",
    "# d14cm2 = d14cm2.set_index('ID')\n",
    "# mod2_profid = d14cm2.index\n",
    "z, dz, zsoih = isam.get_isam_soildp(10)\n",
    "\n",
    "# Extract corresponding measurements for each site and make figure\n",
    "data.nodedepth = data.Layer_top + (data.Layer_bottom - data.Layer_top)/2.\n",
    "myarray = d14cm.index.unique()\n",
    "for i in myarray:\n",
    "    d14co = data.D14C_BulkLayer.loc[i]\n",
    "    nd = data.nodedepth.loc[i]\n",
    "    if(d14co.__class__.__name__ == 'float64'):\n",
    "        Xobs = d14co   # SOC profile kgCm-3\n",
    "        Yobs = nd      # 1cm to 200cm\n",
    "    else:\n",
    "        Xobs = d14co.as_matrix()   # SOC profile kgCm-3\n",
    "        Yobs = nd.as_matrix()     # 1cm to 200cm\n",
    "    Xmod = d14cm.loc[i].as_matrix()   # SOC profile kgCm-3\n",
    "    Ymod = z*100.     # 1cm to 200cm\n",
    "    # Xmod2 = d14cm2.loc[i].as_matrix()   # SOC profile kgCm-3\n",
    "    # Ymod2 = z*100.     # 1cm to 200cm\n",
    "    if (data.Site[i].__class__.__name__ == 'Series'):\n",
    "        tit = data.Site[i].unique().astype('string')[0]\n",
    "    else:\n",
    "        tit = data.Site[i].encode('ascii','ignore')\n",
    "    path = './Figs_obsvsmod_calibrate/'+str(i)+'_'+tit+'.png'\n",
    "    xticks = (-1000, -800, -600, -400, -200, 0, 200)\n",
    "    #status = socplt.plot_obsvsmod(Xobs, Yobs, Xmod, Ymod, tit, path, xticks)\n",
    "    status = socplt.plot_obsvsmod(Xobs, Yobs, Xmod, Ymod, tit, path, None, None, xticks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
